{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4cf422b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Handle Null Values\n",
    "- Drop Columns\n",
    "- Drop rows\n",
    "- Various parameters in Dropping functionalities\n",
    "- Handling missing values by mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78589aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handle Null Values</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22334d38350>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Handle Null Values\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d15a5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('inferSchema', 'true').option('sep', '\\t').option('header', 'true').csv('data/testv.csv')\n",
    "# df.show(10, truncate=False)  # Show the first 10 rows of the DataFrame without truncating the output\n",
    "\n",
    "# df.write.csv('./data1.csv', header=True, mode='overwrite')  # Write the DataFrame to a CSV file with header and overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ee4cdf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MSISDN: integer (nullable = true)\n",
      " |-- NID: string (nullable = true)\n",
      " |-- EMAIL: string (nullable = true)\n",
      " |-- POSTPAID_TARIFF: string (nullable = true)\n",
      " |-- PHONE_NUMBER: string (nullable = true)\n",
      " |-- GEN_NID: string (nullable = true)\n",
      " |-- new: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "new_df = df.select(\n",
    "    col('MSISDN'),\n",
    "    col('NID'),\n",
    "    col('EMAIL'),\n",
    "    col('POSTPAID_TARIFF')\n",
    ").withColumn(\n",
    "    'PHONE_NUMBER',\n",
    "    concat(lit('230'), col('MSISDN'))\n",
    ").withColumn(\n",
    "    'GEN_NID',\n",
    "    col('NID').cast('string')\n",
    ").withColumn(\n",
    "    'new',\n",
    "    col('NID').cast('integer')\n",
    ")\n",
    "\n",
    "# Select specific columns and show the first 10 rows without truncating the output\n",
    "\n",
    "new_df.printSchema()  # Print the schema of the DataFrame to see the structure and data types\n",
    "# new_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19b25d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = new_df.drop('GEN_NID')\n",
    "# dropped_df.show(5, truncate=False)  # Show the first 5 rows of the DataFrame after dropping the 'GEN_NID' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c257299",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "# ðŸ“Œ Handling NULL (EMPTY) Values in PySpark\n",
    "-------------------------------------------------------------\n",
    "\n",
    "Null (or None) values represent missing or undefined data in a DataFrame.\n",
    "They can lead to incorrect results during analysis or computation.\n",
    "Best practice is to clean them by removing, replacing, or filling them with values.\n",
    "\n",
    "In PySpark, you can handle nulls via:\n",
    "1. DataFrame methods: df.drop(), df.fill(), df.replace(); This method is a wrapper around the df.na.method()\n",
    "2. DataFrame.na submodule: df.na.drop(), df.na.fill(), df.na.replace\n",
    "\n",
    "**Note**: They are used the same way and you can decide whichever one to use, they do the same thing and are used same way. The df.method() is a wrapper around the df.na.method();\n",
    "But try to stick to the df.na.method() style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810c888-02fc-4ae4-ae3e-2db8402542cd",
   "metadata": {},
   "source": [
    "### ðŸ”¸ Drop Rows with Nulls: using df.Method() and df.na.method()\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0771130-f29f-4261-ad4f-fa14fe313792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|  MSISDN|           NID|               EMAIL|POSTPAID_TARIFF|PHONE_NUMBER| new|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|54924133|J0711874907942|JEANPOMPIER0517@G...|           NULL| 23054924133|NULL|\n",
      "|54846497|B160985220041C|                NULL|           NULL| 23054846497|NULL|\n",
      "|57369115|      S0987933|                NULL|           NULL| 23057369115|NULL|\n",
      "|57113437|D090290300772B|                NULL|           NULL| 23057113437|NULL|\n",
      "|58468805|N110561330075F|                NULL|           NULL| 23058468805|NULL|\n",
      "|57228141|     A03228083|                NULL|           NULL| 23057228141|NULL|\n",
      "|57119226|P0502893808515|                NULL|           NULL| 23057119226|NULL|\n",
      "|57118074|M1503654401680|                NULL|           NULL| 23057118074|NULL|\n",
      "|54881304|J080295290173A|                NULL|           NULL| 23054881304|NULL|\n",
      "|57320678|L100783300094F|                NULL|           NULL| 23057320678|NULL|\n",
      "|70419962|     PAG898953|                NULL|           NULL| 23070419962|NULL|\n",
      "|54882497|J0204981200574|                NULL|           NULL| 23054882497|NULL|\n",
      "|57432871|P0407853002537|                NULL|           NULL| 23057432871|NULL|\n",
      "|54599755|G0602930101634|                NULL|           NULL| 23054599755|NULL|\n",
      "|54521567|L141089380213G|                NULL|           NULL| 23054521567|NULL|\n",
      "|54537767|S0208991502950|                NULL|           NULL| 23054537767|NULL|\n",
      "|54591599|F3008534119942|                NULL|           NULL| 23054591599|NULL|\n",
      "|70430321|     NYRL44185|                NULL|           NULL| 23070430321|NULL|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… Using df.na.drop()\n",
    "# dropped_df.na.drop().show()                   # Drop rows with any null; Same as df.na.drop(how='any').show()\n",
    "# dropped_df.na.drop(how=\"all\").show()          # Drop rows where all columns are null\n",
    "# dropped_df.na.drop(subset=[\"email\"]).show()   # Drop rows where 'email' field is null\n",
    "dropped_df.na.drop(thresh=2).show()             # Keeps rows with at least 2 or more non-null values; Rows with less than 2 non-values are removed\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Using df.dropna() (direct); Absolutely same way you use the df.na.drop\n",
    "# dropped_df.dropna().show()\n",
    "# dropped_df.dropna(how=\"all\").show()\n",
    "# dropped_df.dropna(subset=[\"email\"]).show()\n",
    "# dropped_df.dropna(thresh=2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f2728-0a67-43d4-909f-b32727211b99",
   "metadata": {},
   "source": [
    "### ðŸ”¸ Fill Null Values\n",
    "---\n",
    "\n",
    "Replace NULL values with static value, make sure the value matches the column's data type.\n",
    "This method does not fill/replace a value based on computation or expression, instead use the withColumn and when().then() / coalesce().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "60b8fa2a-fd43-4c1f-aef3-48c05c598015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+---------------+------------+---+\n",
      "|  MSISDN|           NID|               EMAIL|POSTPAID_TARIFF|PHONE_NUMBER|new|\n",
      "+--------+--------------+--------------------+---------------+------------+---+\n",
      "|54924133|J0711874907942|JEANPOMPIER0517@G...|          EMPTY| 23054924133|  0|\n",
      "|54846497|B160985220041C|      test@gmail.com|          EMPTY| 23054846497|  0|\n",
      "|57369115|      S0987933|      test@gmail.com|          EMPTY| 23057369115|  0|\n",
      "|57113437|D090290300772B|      test@gmail.com|          EMPTY| 23057113437|  0|\n",
      "|58468805|N110561330075F|      test@gmail.com|          EMPTY| 23058468805|  0|\n",
      "|57228141|     A03228083|      test@gmail.com|          EMPTY| 23057228141|  0|\n",
      "|57119226|P0502893808515|      test@gmail.com|          EMPTY| 23057119226|  0|\n",
      "|57118074|M1503654401680|      test@gmail.com|          EMPTY| 23057118074|  0|\n",
      "|54881304|J080295290173A|      test@gmail.com|          EMPTY| 23054881304|  0|\n",
      "|57320678|L100783300094F|      test@gmail.com|          EMPTY| 23057320678|  0|\n",
      "|70419962|     PAG898953|      test@gmail.com|          EMPTY| 23070419962|  0|\n",
      "|54882497|J0204981200574|      test@gmail.com|          EMPTY| 23054882497|  0|\n",
      "|57432871|P0407853002537|      test@gmail.com|          EMPTY| 23057432871|  0|\n",
      "|54599755|G0602930101634|      test@gmail.com|          EMPTY| 23054599755|  0|\n",
      "|54521567|L141089380213G|      test@gmail.com|          EMPTY| 23054521567|  0|\n",
      "|54537767|S0208991502950|      test@gmail.com|          EMPTY| 23054537767|  0|\n",
      "|54591599|F3008534119942|      test@gmail.com|          EMPTY| 23054591599|  0|\n",
      "|70430321|     NYRL44185|      test@gmail.com|          EMPTY| 23070430321|  0|\n",
      "+--------+--------------+--------------------+---------------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… Using df.na.fill()\n",
    "# dropped_df.na.fill(0).show()             # Fill all numeric columns with null values with integer value 0\n",
    "# dropped_df.na.fill(\"Unknown\").show()     # Fill all string columns with null values with string value \"Unknown\"\n",
    "\n",
    "\n",
    "# Fill specified column's (dict key) null value with a static value (dict value)\n",
    "dropped_df.na.fill({\"EMAIL\":\"test@gmail.com\", \"POSTPAID_TARIFF\": \"EMPTY\", \"NEW\": 0.0}).show()    \n",
    "\n",
    "\n",
    "# âœ… Using df.fillna(): Same as the df.na.fill above\n",
    "# dropped_df.fillna(0).show()\n",
    "# dropped_df.fillna(\"Unknown\").show()\n",
    "# dropped_df.fillna({\"EMAIL\":\"test@gmail.com\", \"POSTPAID_TARIFF\": \"EMPTY\", \"NEW\": 0.0}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bd1f0-3cee-427c-9e80-321e89187aae",
   "metadata": {},
   "source": [
    "### ðŸ”¸ Replace Specific Values,\n",
    "---\n",
    "\n",
    "Used to replace explicit values (non-null), across the dataframe; \n",
    "That is you can replace 'Wisdom' to something else, or 2302 to 0 or -00 ...\n",
    "- It is specifically used for changing a value to another value or Null, \n",
    "- it is not used for changing NULL to a different value (even if it has same type). Use na.fill() or df.fill() instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9cd862d2-4c9d-4aee-b445-36c709fea69e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|  MSISDN|           NID|               EMAIL|POSTPAID_TARIFF|PHONE_NUMBER| new|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|54924133|J0711874907942|JEANPOMPIER0517@G...|           NULL|      Value2|NULL|\n",
      "|54846497|B160985220041C|                NULL|           NULL| 23054846497|NULL|\n",
      "|57369115|        Value1|                NULL|           NULL| 23057369115|NULL|\n",
      "|57113437|D090290300772B|                NULL|           NULL| 23057113437|NULL|\n",
      "|58468805|N110561330075F|                NULL|           NULL| 23058468805|NULL|\n",
      "|57228141|     A03228083|                NULL|           NULL| 23057228141|NULL|\n",
      "|57119226|P0502893808515|                NULL|           NULL| 23057119226|NULL|\n",
      "|57118074|M1503654401680|                NULL|           NULL| 23057118074|NULL|\n",
      "|54881304|J080295290173A|                NULL|           NULL| 23054881304|NULL|\n",
      "|57320678|L100783300094F|                NULL|           NULL| 23057320678|NULL|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… Using df.na.replace()\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Basic usage\n",
    "# dropped_df.na.replace(\"B160985220041C\", None)\n",
    "\n",
    "# Complex usage: the length of 'to_replace' must match lenght or 'value' \n",
    "#   index 0 in 'to_replace' list represents index 0 in 'value' list on subset (column) NID and PHONE_NUMBER.\n",
    "dropped_df.na.replace(\n",
    "    to_replace=[\"S0987933\", \"23054924133\"], value=[\"Value1\", \"Value2\"], subset=[\"NID\", \"PHONE_NUMBER\"]\n",
    ").show(10, vertical=False)\n",
    "# .collec()\n",
    "\n",
    "# âœ… Using df.replace()\n",
    "# dropped_df.replace(\"B160985220041C\", None)\n",
    "# dropped_df.replace(to_replace=[\"S0987933\", \"23054924133\"], value=[\"Value1\", \"Value2\"], subset=[\"NID\", \"PHONE_NUMBER\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf33b9-3478-4ade-b8df-e91120e6f530",
   "metadata": {},
   "source": [
    "### ðŸ”¸ Using SQL Functions for Conditional Fill\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "520f63b3-20de-4f74-ac9a-72381b5b2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|  MSISDN|           NID|               EMAIL|POSTPAID_TARIFF|PHONE_NUMBER| new|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|54924133|J0711874907942|JEANPOMPIER0517@G...|           NONE| 23054924133|NULL|\n",
      "|54846497|B160985220041C|     sample@test.com|           NONE| 23054846497|NULL|\n",
      "|57369115|      S0987933|     sample@test.com|           NONE| 23057369115|NULL|\n",
      "|57113437|D090290300772B|     sample@test.com|           NONE| 23057113437|NULL|\n",
      "|58468805|N110561330075F|     sample@test.com|           NONE| 23058468805|NULL|\n",
      "|57228141|     A03228083|     sample@test.com|           NONE| 23057228141|NULL|\n",
      "|57119226|P0502893808515|     sample@test.com|           NONE| 23057119226|NULL|\n",
      "|57118074|M1503654401680|     sample@test.com|           NONE| 23057118074|NULL|\n",
      "|54881304|J080295290173A|     sample@test.com|           NONE| 23054881304|NULL|\n",
      "|57320678|L100783300094F|     sample@test.com|           NONE| 23057320678|NULL|\n",
      "|70419962|     PAG898953|     sample@test.com|           NONE| 23070419962|NULL|\n",
      "|54882497|J0204981200574|     sample@test.com|           NONE| 23054882497|NULL|\n",
      "|57432871|P0407853002537|     sample@test.com|           NONE| 23057432871|NULL|\n",
      "|54599755|G0602930101634|     sample@test.com|           NONE| 23054599755|NULL|\n",
      "|54521567|L141089380213G|     sample@test.com|           NONE| 23054521567|NULL|\n",
      "|54537767|S0208991502950|     sample@test.com|           NONE| 23054537767|NULL|\n",
      "|54591599|F3008534119942|     sample@test.com|           NONE| 23054591599|NULL|\n",
      "|70430321|     NYRL44185|     sample@test.com|           NONE| 23070430321|NULL|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|  MSISDN|           NID|               EMAIL|POSTPAID_TARIFF|PHONE_NUMBER| new|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "|54924133|J0711874907942|JEANPOMPIER0517@G...|           NULL| 23054924133|NULL|\n",
      "|54846497|B160985220041C|         23054846497|           NULL| 23054846497|NULL|\n",
      "|57369115|      S0987933|         23057369115|           NULL| 23057369115|NULL|\n",
      "+--------+--------------+--------------------+---------------+------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+--------------+-----+---------------+------------+----+\n",
      "|  MSISDN|           NID|EMAIL|POSTPAID_TARIFF|PHONE_NUMBER| new|\n",
      "+--------+--------------+-----+---------------+------------+----+\n",
      "|54846497|B160985220041C| NULL|           NULL| 23054846497|NULL|\n",
      "|57369115|      S0987933| NULL|           NULL| 23057369115|NULL|\n",
      "|57113437|D090290300772B| NULL|           NULL| 23057113437|NULL|\n",
      "|58468805|N110561330075F| NULL|           NULL| 23058468805|NULL|\n",
      "|57228141|     A03228083| NULL|           NULL| 23057228141|NULL|\n",
      "|57119226|P0502893808515| NULL|           NULL| 23057119226|NULL|\n",
      "|57118074|M1503654401680| NULL|           NULL| 23057118074|NULL|\n",
      "|54881304|J080295290173A| NULL|           NULL| 23054881304|NULL|\n",
      "|57320678|L100783300094F| NULL|           NULL| 23057320678|NULL|\n",
      "|70419962|     PAG898953| NULL|           NULL| 23070419962|NULL|\n",
      "|54882497|J0204981200574| NULL|           NULL| 23054882497|NULL|\n",
      "|57432871|P0407853002537| NULL|           NULL| 23057432871|NULL|\n",
      "|54599755|G0602930101634| NULL|           NULL| 23054599755|NULL|\n",
      "|54521567|L141089380213G| NULL|           NULL| 23054521567|NULL|\n",
      "|54537767|S0208991502950| NULL|           NULL| 23054537767|NULL|\n",
      "|54591599|F3008534119942| NULL|           NULL| 23054591599|NULL|\n",
      "|70430321|     NYRL44185| NULL|           NULL| 23070430321|NULL|\n",
      "+--------+--------------+-----+---------------+------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, coalesce\n",
    "\n",
    "# Replace nulls in 'email' columns using condition\n",
    "dropped_df.withColumn(\n",
    "    \"EMAIL\",\n",
    "    col=when(col(\"EMAIL\").isNull(), \"sample@test.com\").otherwise(col(\"EMAIL\"))\n",
    ").withColumn(\n",
    "    \"POSTPAID_TARIFF\",\n",
    "    when(col(\"POSTPAID_TARIFF\").isNull(), \"NONE\").otherwise(col(\"POSTPAID_TARIFF\"))\n",
    ").show()\n",
    "\n",
    "\n",
    "# Choose first non-null from multiple columns\n",
    "#    coalesce() returns the first non-null value. coalesce(NULL, NULL, 1, 2) => 1\n",
    "\n",
    "dropped_df.withColumn(\n",
    "    \"EMAIL\",\n",
    "    coalesce(col(\"EMAIL\"), col(\"PHONE_NUMBER\"))\n",
    ").show(3)\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… Quick Summary\n",
    "# ----------------------------\n",
    "\n",
    "# Method              | Description\n",
    "#---------------------|--------------------------------------------\n",
    "# df.na.drop()        | Drop rows with nulls\n",
    "# df.na.fill()        | Fill nulls with value or dict\n",
    "# df.na.replace()     | Replace values (can include null)\n",
    "# df.dropna()         | Shorthand for df.na.drop()\n",
    "# df.fillna()         | Shorthand for df.na.fill()\n",
    "# df.replace()        | Shorthand for df.na.replace()\n",
    "# when().then() / coalesce()     | Conditional null handling\n",
    "\n",
    "# # ----------------------------\n",
    "# # ðŸ§ª Tips\n",
    "# # ----------------------------\n",
    "\n",
    "# from pyspark.sql.functions import count, when\n",
    "\n",
    "# - Always check schema: df.printSchema()\n",
    "# - Profile nulls per column: df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# - Use coalesce() to handle priority fallback among columns\n",
    "\n",
    "# # Check rows with any nulls\n",
    "# dropped_df.filter(\"email IS NULL\").show()\n",
    "# dropped_df.select(col(\"EMAIL\")).filter(\"EMAIL is NULL\").count()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
