{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c85368-4fc8-4eec-98dc-343b1dc7418a",
   "metadata": {},
   "source": [
    "## Topics:\n",
    "- Dataframe\n",
    "- Reading the data from CSV\n",
    "- Checking the datatypes of the columns in the Dataframe and also Infer schema with the inferSchema' argument in ...read.csv(...) / .option(...).csv(...) method.\n",
    "- Select columns and indexing\n",
    "- Check describe option similar to pandas\n",
    "- Adding new columns\n",
    "- Dropping columns\n",
    "- Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1f6fe3aa-1111-4d84-a9f5-20f759afb6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://BANWACHWI.nova.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkOperation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bcb16ab0d0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkOperation').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff8283f9-1459-4866-9b32-5a0f143b951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/DND_20250708.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "085b810c-166b-4b13-9c72-8012903e1791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NUMBER: string (nullable = true)\n",
      " |-- TE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema of the dataframe\n",
    "df.printSchema() # Gives a tree structure of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "47e18cf7-aeca-4a61-a747-55f3d3fbc8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NUMBER', 'TE']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all columns in the dataframe, outputs a python list with string values.\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "189ed312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|  NUMBER|  TE|\n",
      "+--------+----+\n",
      "|54283248|  89|\n",
      "|54283299|3234|\n",
      "|54283313|  43|\n",
      "|54283321|  23|\n",
      "|54283332| 232|\n",
      "+--------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------------------+------------------+\n",
      "|summary|             NUMBER|                TE|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|              55570|                 5|\n",
      "|   mean|5.700295175958753E7|             724.2|\n",
      "| stddev|  4352739.084227369|1405.3959228630201|\n",
      "|    min|            5713508|                23|\n",
      "|    max|             NUMBER|              3234|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show dataframe and rows\n",
    "df.show(5)  # Show the first 5 rows\n",
    "\n",
    "df.head(5)  # Get the first 5 rows as a list of Row objects\n",
    "\n",
    "df.count()  # Count the number of rows in the DataFrame\n",
    "\n",
    "df.tail(5)  # Get the last 5 rows as a list of Row objects\n",
    "\n",
    "df.describe().show()  # Get summary statistics for numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ee67803d-a3ea-4016-b6d0-11feacd4a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|PHONE_NUMBER|  TE|\n",
      "+------------+----+\n",
      "|   54283.248|  89|\n",
      "|   54283.299|3234|\n",
      "|   54283.313|  43|\n",
      "|   54283.321|  23|\n",
      "|   54283.332| 232|\n",
      "|   54283.333|NULL|\n",
      "|   54283.335|NULL|\n",
      "|   54283.354|NULL|\n",
      "|   54283.355|NULL|\n",
      "|   54283.367|NULL|\n",
      "+------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECTING COLUMNS:\n",
    "\n",
    "# df.columnName #  Works only if column name is a valid Python identifier (e.g. no spaces, special characters).\n",
    "# df['columnName'] # This is more robust and supports any column name (even with spaces or special chars).\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "# df.select(col(\"NUMBER\"), col('NUMBER'))# col(): Preferred when chaining expressions like transformations, filters, or with withColumn().\n",
    "# df.select('NUMBER', 'NUMBER') # to return a DataFrame with one or more columns\n",
    "\n",
    "'''\n",
    "# df.columns[0] # using the index to access the first column, from the .columns list; \n",
    "# df.select(df.columns[0])\n",
    "'''\n",
    "\n",
    "df.select(\n",
    "    (col('NUMBER') / 1000).alias('PHONE_NUMBER'),\n",
    "    col('TE').alias('TE')\n",
    "    ).show(10)  # Show the first 3 rows of the 'NUMBER' column with an alias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NUMBER', 'string'), ('TE', 'int')] \n",
      "\n",
      "StructType([StructField('NUMBER', StringType(), True), StructField('TE', IntegerType(), True)]) \n",
      "\n",
      "root\n",
      " |-- NUMBER: string (nullable = true)\n",
      " |-- TE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes, \"\\n\") # Get the data types of each column in the DataFrame as a list of tuples [(column_name, data_type), ...]\n",
    "print(df.schema, \"\\n\")  # Get the schema of the DataFrame, which includes column names and data types\n",
    "df.printSchema()  # Print the schema of the DataFrame in a tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e29fad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------+\n",
      "|  NUMBER|  TE|  Generated|\n",
      "+--------+----+-----------+\n",
      "|54283248|  89|23054283248|\n",
      "|54283299|3234|23054283299|\n",
      "+--------+----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to the DataFrame\n",
    "from pyspark.sql.functions import lit, concat\n",
    "d = df.withColumn(colName='Generated', col=concat(lit('230'), col(\"NUMBER\").cast('string'))) # Add a new column 'Generated' by concatenating '230' with the 'NUMBER' column\n",
    "\n",
    "d.schema  # Show the DataFrame with the new column\n",
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "693bfebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  NUMBER|\n",
      "+--------+\n",
      "|54283248|\n",
      "|54283299|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+\n",
      "|PHONE_NUMBER|\n",
      "+------------+\n",
      "|    54283248|\n",
      "|    54283299|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping a column from the DataFrame\n",
    "\n",
    "dropped_df = d.drop('Generated', 'Te')  # Drop duplicates based on the 'Generated Column'\n",
    "dropped_df.show(2)  # Show the first 5 rows of the DataFrame after dropping duplicates\n",
    "\n",
    "# Renaming a column in the DataFrame\n",
    "\n",
    "renamed_df = dropped_df.withColumnRenamed(existing='NUMBER', new='PHONE_NUMBER')  # Rename the 'NUMBER' column to 'PHONE_NUMBER'\n",
    "renamed_df.show(2)  # Show the first 5 rows of the DataFrame after renaming the column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
