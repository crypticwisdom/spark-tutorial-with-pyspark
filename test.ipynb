{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89a109f",
   "metadata": {},
   "source": [
    "- Understand spark configuration tuning for better performance during data processing.\n",
    "- Undertand the amount of resources needed for a Job.\n",
    "- Use increasing job loads for test purposes (500k -> 1M -> 2M records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3917fda",
   "metadata": {},
   "source": [
    "# Streaming:\n",
    ".maxfile per trigger, 1 -> set the number of file when spart streaming is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50543ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"streaming\")\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio-service:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "      .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #   .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\")\n",
    "      .config(\"spark.jars\", \"/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/delta-core_2.12-2.4.0.jar,/opt/spark/jars/delta-storage-2.4.0.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "schemaTypes = {\n",
    "    \"MSISDN\": \"String\", \"NID\": \"String\", \"NID_LEN\": \"Number\", \"CUSTOMERIDNAME\": \"String\", \"EMAIL\": \"String\",\n",
    "    \"GROSS_DATE\": \"Date\", \"CUSTOMER_TYPE\": \"String\", \"USERTYPE\": \"String\", \"SUBSCRIBERCAT\": \"String\",\n",
    "    \"SUBSCRIBERSUBCAT\": \"String\", \"SEGMENT\": \"String\", \"POSTPAID_TARIFF\": \"String\", \"POSTPAID_MAINPRODUCT\": \"String\",\n",
    "    \"POSTPAID_SUBPRODUCT\": \"String\", \"NATIONALITY\": \"String\", \"AON_DAYS\": \"Number\", \"AON_MONTH\": \"Number\",\n",
    "    \"AON_YEAR\": \"Number\", \"ALTERNATE_MOB_NUM\": \"String\", \"FAVOURITE_LOCATION\": \"String\", \"DAYS_INACT\": \"Number\",\n",
    "    \"BAL_1\": \"Number\", \"BAL_2\": \"Number\", \"BAL_3\": \"Number\", \"DATA_USERS\": \"String\",\n",
    "    \"PACK_USERS\": \"String\", \"PACK_90D\": \"Number\", \"PACK_30D\": \"Number\", \"DAILY_PACK_90D\": \"Number\", \"DAILY_PACK_30D\": \"Number\",\n",
    "    \"WEEKLY_PACK_90D\": \"Number\", \"WEEKLY_PACK_30D\": \"Number\", \"MONTHLY_PACK_90D\": \"Number\", \"MONTHLY_PACK_30D\": \"Number\",\n",
    "    \"RECHARGE_VAL_30D\": \"Number\", \"RECHARGE_VAL_D1\": \"Number\", \"RECHARGE_VAL_D2\": \"Number\", \"RECHARGE_VAL_D3\": \"Number\",\n",
    "    \"AIRTIME_USG\": \"Number\", \"YOUTH_PACK\": \"String\", \"TOTAL_ARPU\": \"Number\", \"ONNET_REVENUE\": \"Number\", \"XNET_REVENUE\": \"Number\",\n",
    "    \"ONNET_TOTAL_MINS_90D\": \"Number\", \"ONNET_TOTAL_MINS_30D\": \"Number\", \"XNET_TOTAL_MINS_90D\": \"Number\", \"XNET_TOTAL_MINS_30D\": \"Number\",\n",
    "    \"VOICE_INCOMING_ARPU\": \"Number\", \"SMS_ARPU\": \"Number\", \"DATA_USG_GB_90D\": \"Number\", \"DATA_USG_GB_30D\": \"Number\",\n",
    "    \"DATA_PAYG_USG_GB_90D\": \"Number\", \"DATA_PAYG_USG_GB_30D\": \"Number\", \"TOTAL_DATA_ARPU_90D\": \"Number\", \"TOTAL_DATA_ARPU_30D\": \"Number\",\n",
    "    \"TOTAL_DATA_PAYG_ARPU_90D\": \"Number\", \"TOTAL_DATA_PAYG_ARPU_30D\": \"Number\", \"ILD_ARPU\": \"Number\", \"ILD_SMS_ARPU\": \"Number\", \"DEVICE_SUBTYPE_1\": \"String\",\n",
    "    \"DEVICE_SUBTYPE_2\": \"String\", \"DEVICE_SUBTYPE_3\": \"String\", \"OS_SYSTEM_1\": \"String\", \"OS_SYSTEM_2\": \"String\", \"OS_SYSTEM_3\": \"String\",\n",
    "    \"TECHNOLOGY_1\": \"String\",\"TECHNOLOGY_2\": \"String\",\"TECHNOLOGY_3\": \"String\",\"SIM_TYPE_1\": \"String\",\n",
    "    \"SIM_TYPE_2\": \"String\", \"SIM_TYPE_3\": \"String\", \"DUAL_SIM_1\": \"String\", \"DUAL_SIM_2\": \"String\", \"DUAL_SIM_3\": \"String\",\n",
    "    \"ESIM_1\": \"String\", \"ESIM_2\": \"String\", \"ESIM_3\": \"String\"\n",
    "}\n",
    "\n",
    "def spark_type(dtype):\n",
    "    if dtype == \"String\":\n",
    "        return StringType()\n",
    "    elif dtype == \"Number\":\n",
    "        return FloatType()\n",
    "    elif dtype == \"Date\":\n",
    "        return StringType()\n",
    "    return StringType()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name, spark_type(dtype), True)\n",
    "    for name, dtype in schemaTypes.items()\n",
    "])\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n",
    "\n",
    "# sf = (\n",
    "#     spark.read.parquet(\"s3a://batch-etl-pipeline/final/part-00000-31f26158-5a47-4602-8a52-bc7563c2ff3f-c000.snappy.parquet\")\n",
    "# )\n",
    "# print(\"\\n\\n\\n\\n\\n\\n\", sf.rdd.getNumPartitions(), \"\\n\\n\\n\\n\\n\\n\")  # This will return the number of partitions in the DataFrame.\n",
    "# sf.show(2)  # This will show the first 2 rows of the DataFrame.\n",
    "# print(\"\\n\\n\\n\\n\\n\\n\", sf.rdd.getNumPartitions(), \"\\n\\n\\n\\n\\n\\n\")  # This will return the number of partitions in the DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "# sf.write.parquet(\"s3a://batch-etl-pipeline/3i\")\n",
    "# exit()\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Process one file at a time\n",
    "    .load(\"s3a://batch-etl-pipeline/zain/\")\n",
    ")\n",
    "\n",
    "# df.rdd.getNumPartitions()\n",
    "# df.show() # ❌ Not allowed for streaming DataFrame\n",
    "# In structured streaming, you must always use .writeStream.start(), never use .show() or .collect(), or you’ll hit OOM.\n",
    "# ✅ foreachBatch is the only way to inspect DataFrame.rdd in streaming.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Just like spark.read(...), the spark.readStream(...) method reads data in parallel by splitting input files into partitions.\n",
    "df.rdd.getNumPartitions() -> Returns the number of partitions in the DataFrame.\n",
    "\n",
    "\n",
    "* ✅ **`.repartition`**: This **explicitly forces a cluster-wide shuffle** to redistribute data **before the next transformation** (or action).\n",
    "* ✅ **`.coalesce`**: This **reduces the number of partitions**, **avoiding a full shuffle** by merging partitions where possible with minimal movement..\n",
    "* ✅ **`spark.sql.shuffle.partitions`**: This controls the **default number of partitions** used **after wide transformations** (like `groupBy`, `join`, `distinct`, etc.) **when you don't specify partitioning explicitly**.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n",
    "\n",
    "---\n",
    "\n",
    "Min partitions from sparkContext.defaultParallelism\n",
    "> On local mode: usually = number of cores.\n",
    "> On cluster mode: total number of cores across the cluster.\n",
    "\n",
    "If you're reading a single 193MB CSV file and you're seeing 8 partitions, it's most likely because:\n",
    "spark.sparkContext.defaultParallelism == 8\n",
    "\n",
    "---\n",
    "\n",
    ".repartition(n) → Controls number of partitions before a shuffle\n",
    "spark.sql.shuffle.partitions → Controls number of shuffle output partitions during wide transformations\n",
    "\n",
    "---\n",
    "\n",
    "spark.sql.files.maxPartitionBytes\t128MB\t\n",
    "The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\n",
    "\n",
    "\n",
    "spark.default.parallelism:  Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user.\t\n",
    "Local mode: number of cores on the local machine\n",
    "Others: total number of cores on all executor nodes or 2, whichever is larger\n",
    "\n",
    "\n",
    "\n",
    "🔹 When Should You Repartition?\n",
    "Too few partitions? → Not enough parallelism → poor CPU utilization.\n",
    "\n",
    "Too many small partitions? → Overhead from task scheduling and I/O.\n",
    "\n",
    "Good rule of thumb: aim for 100–200 MB per partition for optimal performance.\n",
    "\"\"\"\n",
    "\n",
    "print(spark.sparkContext.defaultParallelism, \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\")  # This will print the default parallelism level, which is usually equal to the number of cores available in the cluster.\n",
    "# spark.sparkContext.defaultParallelism  # This will also return the default parallelism level.\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "def inspect_partition_count(batch_df, batch_id):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n\\n\\n✅ Batch {batch_id} - Num Partitions: {batch_df.rdd.getNumPartitions()}\")\n",
    "    # batch_df = batch_df.repartition(2)\n",
    "    batch_df = batch_df.coalesce(3)\n",
    "    batch_df.collect()  # This will trigger the computation and allow you to see the number of partitions.\n",
    "    batch_df = batch_df.groupBy(\"NID\").count()\n",
    "\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n\\n\\n✅ ---- Batch {batch_id} - Num Partitions: {batch_df.rdd.getNumPartitions()} \\n\\n\\n\\n\")\n",
    "    batch_df.show(2)\n",
    "    # # batch_df.select(\"MSISDN\").show()  # Example operation to inspect data\n",
    "    # phon = batch_df.withColumn(\"Phone\", concat(lit(\"230\"), col(\"MSISDN\")))  # Example operation to write data\n",
    "    # phon.select(\"Phone\").show(10)\n",
    "\n",
    "\n",
    "\n",
    "# If you're using streaming, you must use .writeStream to start the query.\n",
    "query = (df.writeStream\n",
    "    .format(\"console\")\n",
    "    .foreachBatch(inspect_partition_count)\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"s3a://batch-etl-pipeline/checkpoints1/upsert-csv/\")\n",
    "    .trigger(processingTime=f\"10 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# query = (\n",
    "#     df.writeStream\n",
    "#     .format(\"mongodb\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .option(\"checkpointLocation\", \"s3a://batch-etl-pipeline/checkpoints/upsert-csv/\")\n",
    "#     .option(\"spark.mongodb.connection.uri\", \"mongodb://host.docker.internal:27017/\")\n",
    "#     .option(\"spark.mongodb.database\", \"backend\")        # ✅ specify your DB\n",
    "#     .option(\"spark.mongodb.collection\", \"new\")    # ✅ specify your collection\n",
    "#     .trigger(processingTime=\"10 seconds\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark-submit --master spark://spark-master:7077 --deploy-mode client --jars / /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/delta-core_2.12-2.4.0.jar,/opt/spark/jars/delta-storage-2.4.0.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar test.py\n",
    "\n",
    "# MINIO_INPUT_PATH=s3a://batch-etl-pipeline/split-csvs/\n",
    "# CHECKPOINT_LOCATION=s3a://batch-etl-pipeline/checkpoints/upsert-csv/\n",
    "\n",
    "# MONGO_USER=EMTEL_CAMPAIGN_PROD\n",
    "# MONGO_PASSWORD=Emt3lCaMpa1gN0dC\n",
    "# MONGO_HOSTS=172.26.64.42:27017,172.26.64.44:27017,172.26.64.43:27017,172.26.64.162:27017,172.26.64.163:27017\n",
    "# MONGO_PORT=27017\n",
    "# MONGO_DB=EMTEL_CAMPAIGN_PROD\n",
    "# MONGO_COLLECTION=userdatas-test\n",
    "# REPLICA_SET_NAME=EMTEL\n",
    "\n",
    "# MINIO_S3A_ACCESS_KEY=minioadmin\n",
    "# MINIO_S3A_SECRET_KEY=minioadmin\n",
    "\n",
    "# SPARK_APP_NAME=CSV-Upsert-Job-UAT\n",
    "# SPARK_JARS=/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/delta-core_2.12-2.4.0.jar,/opt/spark/jars/delta-storage-2.4.0.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar\n",
    "\n",
    "# STREAMING_PROCESSING_TIME=20\n",
    "# REPARTITION_COUNT=200\n",
    "\n",
    "# df = spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(\"\").load(\"path/to/input/directory\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --deploy-mode client \\\n",
    "  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.2.0 \\\n",
    "  --jars /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/delta-core_2.12-2.4.0.jar,/opt/spark/jars/delta-storage-2.4.0.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar \\\n",
    "  test.py\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
