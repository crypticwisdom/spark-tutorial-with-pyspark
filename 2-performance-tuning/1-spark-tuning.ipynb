{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e88b5ba",
   "metadata": {},
   "source": [
    "# Focuses on Allocating the correct CPU and memory, resource for your spark job.\n",
    "\n",
    "There are logics and rules needed to be followed, in other to optimally size your spark cluster resources, before we can say we need X executors with Y amount of Memory for executors and driver, and Z amount of memory each.\n",
    "\n",
    "This tutorial answers, how we need to do that.\n",
    "- How many executors, memories and cpu cores \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d84bdd",
   "metadata": {},
   "source": [
    "## Cluster Resources:\n",
    "Say we have 5 nodes (machine) in a cluster with:\n",
    "- Each with 12 CPU cores and 48GB RAM.\n",
    "\n",
    "How do we decide the number of executor, core and memory.\n",
    "\n",
    "â€œThin vs. Optimal vs. Fat Executorsâ€ approach is one way practitioners think about allocating executors, cores, and memory in a cluster.\n",
    "\n",
    "Itâ€™s not an official Spark term (you wonâ€™t see it in the docs), but itâ€™s a widely used heuristic in tuning discussions.\n",
    "\n",
    "\n",
    "**There are 3 ways of deciding, which are:**\n",
    "- Thin Executors\n",
    "- Optimal Executors\n",
    "- Fat Executors\n",
    "\n",
    "All 3 of them has advantages and disadvantages.\n",
    "\n",
    "> Leave 1 CPU and 1GB for operation system\n",
    "\n",
    "1. **Fat Executors:** They occupy larger portions of resources available in a single node.\n",
    "- Idea: Fewer executors with many cores and large memory.\n",
    "\n",
    "Given the above resources (5 nodes, 12 cpus and 48gb on each node), so one fat executor is achieved by specifying 11 out of 12 cores and 47GB out of 48GB RAM on each nodes.\n",
    "\n",
    "`spark-submit \n",
    "--num-executor 5 ...\n",
    "--executo-cores 11\n",
    "--exec...memory 47g\n",
    "`\n",
    "So, 5 nodes (machines) will have 5 execs with 11 CPU and 47GB each.\n",
    "\n",
    "\n",
    "#### Truth check for Fact executors:\n",
    "- âœ” True: 11 cores, 47GB â†’ 5 executors = fat executor design.\n",
    "- âŒ Not exactly true: 5 cores, 25GB â†’ still big, but no longer a fat executor in the strict sense â€” itâ€™s a mid/optimal setup.\n",
    "- ðŸ’¡ Fat executors = max resources per executor, but not always the best choice for ETL workloads.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5775f71",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. **Thin executors:** opposite of fat, occupy min, resources.\n",
    "- Idea: Use many executors with fewer cores each (e.g., 1â€“3 cores per executor).\n",
    "\n",
    "1 executor can have 1 core this means it is possible we run 11 executors on each node of 11 CORES.\n",
    "if executor core is 1, num of executors is 11 then the number of memory per executor, will be 47GB/11 approx. 4GB.\n",
    "\n",
    "- 1 node = 11 execs and 4GB.\n",
    "- 5 nodes = 55 execs, 4GB\n",
    "\n",
    "So, we will submit our job to a 5 node cluster of 11 CPU core and 47GB each with:\n",
    "\n",
    "`spark-submit\n",
    "--num-executor 55\n",
    "--exec..core 1\n",
    "--exec..mem... 4g\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0176a72",
   "metadata": {},
   "source": [
    "## Advantages of Fat:\n",
    "- increase task parallelism\n",
    "- Fault tolerance \n",
    "\n",
    "## Disadvantages\n",
    "...\n",
    "\n",
    "\n",
    "## Advantages of Thin:\n",
    "...\n",
    "\n",
    "\n",
    "## Disadvantages of Thin:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b7841",
   "metadata": {},
   "source": [
    "## 3. Optimal Executor Sizing:\n",
    "- Idea: 4â€“5 cores per executor is often the balance between parallelism and GC efficiency.\n",
    "\n",
    "**Rules:**\n",
    "- per node, leave out 1 core and 1GB of ram for Hadoop / yarn / os\n",
    "- Leave out 1 executor or 1 core or 1gb at cluster level.\n",
    "- 3 - 5 cores per executor (good practice)\n",
    "- when you define executor memory it should exclude overhead memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4aeb7",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "Say we have 5 nodes (machine) in a cluster with:\n",
    "- 12 CPU cores and 48GB RAM each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a446276",
   "metadata": {},
   "source": [
    "Let's begin to configure our optimal executors:\n",
    "\n",
    "1. Leave out 1 core and 1gb per node (machine):\n",
    "\n",
    "**Total node-level resource:**\n",
    "`12 core - 1 core = 11 cpu core.`\n",
    "\n",
    "**Total node-level after applying rule 1:**\n",
    "`48gb - 1gb = 47gb`\n",
    "\n",
    "So we are left with 11 cores and 47gb per node.\n",
    "\n",
    "\n",
    "2. Leave out either 1 executor or 1 core or 1 gb for the application at the cluster level.\n",
    "\n",
    "Our total cluster level resource is:\n",
    "\n",
    "-> `total_memory_in_cluster = 5 nodes x 47GB = 235GB`\n",
    "\n",
    "-> `total_cores_in_cluster = 5 nodes x 11 cores = 55 cores`\n",
    "\n",
    "Based on rule 2, subtract 1 CPU and 1 GB, \n",
    "`235GB - 1GB = 234GB`\n",
    "`55 cores - 1 core = 54 cores` \n",
    "\n",
    "`Total cluster resuource = 234 GB and 54 cores at cluster level.`\n",
    "\n",
    "Next, we find the number of executors and memory & core per executor for our Job.\n",
    "\n",
    "1. Assign 3 - 5 cores:\n",
    "going by rule 3, we can choose to use 5 cores per executors.\n",
    "\n",
    "**Find Number of Executors:**\n",
    "`total_executors = 54 cores / 5 cores -> ~ 10 execs.`\n",
    "So in total we would have 10 execs and each node.\n",
    "\n",
    "\n",
    "**Find amount of Memory:**\n",
    "`memory_per_exec = 234 GB / 10 Execs ~ 23.4`, each executor will have ~ 23 GB.\n",
    "\n",
    "Let's apply Rule 4. to get the actual exec. memory.\n",
    "\n",
    "1. Subtract overhead memory, from executor memory.\n",
    "Over head memory should be max of 384MB or 10% of executor memory. max(384MB, 0.10 * memory_per_exec)\n",
    "\n",
    "`actual_mem = 23GB - max(384MB or 10% * 23GB)`\n",
    "10% of 23GB is = 2.3GB\n",
    "`actual_mem = max(384MB, 2.3GB) = 2.3GB`\n",
    "\n",
    ":. 23GB - 2.3GB appr ->  20GB\n",
    "Actual memory per executor = 20GB\n",
    "\n",
    "In summary the Job configuration will be:\n",
    "\n",
    "`number of executors -> 10`\n",
    "`number of cpu per executor -> 5`\n",
    "`number of memory per executor -> 20 GB`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de81d5",
   "metadata": {},
   "source": [
    "# What about the size of DATA?\n",
    "E.G., 10GB, 100GB of data, wouldn't the size of data affect the calculation that has been done above?\n",
    "\n",
    "\n",
    "**Answer:**\n",
    "We have previously calculated the memory per core, we can get this by `executor memory` / `executor cores`.\n",
    "\n",
    "memory_per_core = `20GB / 5 cores = 4GB`\n",
    "\n",
    ":. Memory per 1 core in an executor is 4GB.\n",
    "\n",
    "1 core will hold 4GB of partition, since 1 core processes 1 task/partition, this means, as long as the partition size per core is <= 4GB, 1 core in an executor can process it easily.\n",
    "\n",
    "So the answer revolves around the size of your set partition, default is 128MB, then by this configuration we have made, it will process data seamlessly because 128MB < 4GB. \n",
    "\n",
    "You need to know or control your partition size and your `memory per core` in other to process your data size.\n",
    "\n",
    "ðŸ’¡ Rule of thumb: Aim for ~128 MB (sometimes 200â€“256 MB for large clusters) per partition after compression.\n",
    "Thatâ€™s why you \"need to know your partition size.\"\n",
    "\n",
    "\n",
    "Putting it together\n",
    "\n",
    "When tuning:\n",
    "\n",
    "- Estimate memory per core â†’ decide the maximum safe partition size.\n",
    "- Control partition count (repartition() / coalesce()) so total data size / partition size = number of partitions.\n",
    "- Match number of partitions to cluster parallelism (often 2â€“4Ã— total cores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3bc10",
   "metadata": {},
   "source": [
    "# Practical 2: Optimal Executor Sizing\n",
    "**Using:**\n",
    "3 Nodes, each has 16 cores and 48GB.\n",
    "\n",
    "Calculate the number of executors and cores & memory per executor.\n",
    "\n",
    "**Rule 1: Node level**\n",
    "Remove 1GB and 1 core from each node.\n",
    "remaining: 15 cores and 47GB per executor.\n",
    "\n",
    "**Rule2: Cluster level**\n",
    "cluster resource: 15 x 3 nodes = 45 cores and 47GB x 3 nodes = 141GB\n",
    "Remove 1GB and 1 core for OS operations.\n",
    "\n",
    "remaining: 44 cores and 140GB per executor.\n",
    "\n",
    "**Rule 3:** use 3 - 5 cores per, using 4 cores.\n",
    "num_of_execs = 44 / 4 = 11 execs.\n",
    "number_of_execs_mem = 140 / 11 execs -> aprox. 12GB per execs\n",
    "\n",
    "**Rule 4:** Exclude overhead memory (10% of exec. mem.)\n",
    ":. 10% of 12 = 1.2GB \n",
    "12GB - 1.2 (aprrox 1 GB) = 12 - 1 = 11 GB\n",
    "\n",
    "final memory per executor, after excluding overhead memory is:\n",
    "11GB\n",
    "\n",
    "**Optimal Executor:**\n",
    "--num-exec 11\n",
    "--num-exec-mem 11g\n",
    "--num-core 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf629",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- Start with executor sizing based on hardware, not data size\n",
    "- Then adjust parallelism (partitions) based on data size.\n",
    "- There are many other important Spark optimizations â€” sizing is step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571efaa3",
   "metadata": {},
   "source": [
    "| Area                   | Key Techniques                                                                        |\n",
    "| ---------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **Partitioning**       | Repartition/coalesce wisely, aim for \\~100â€“200MB per partition                        |\n",
    "| **Join Strategy**      | Use `broadcast()` for small tables, understand physical plans (`SortMergeJoin`, etc.) |\n",
    "| **Caching**            | Use `.cache()` or `.persist()` when reusing expensive results                         |\n",
    "| **Shuffle Reduction**  | Avoid wide transformations where possible (groupBy, join, etc.)                       |\n",
    "| **File Format**        | Use **Parquet** or **ORC** instead of CSV for big data                                |\n",
    "| **Predicate Pushdown** | Use filters early so they are pushed into file scan                                   |\n",
    "| **Data Skew**          | Handle skewed keys in joins or aggregations (e.g., salting)                           |\n",
    "| **Cluster Tuning**     | Use dynamic allocation, fine-tune GC (`spark.executor.extraJavaOptions`)              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b75af0",
   "metadata": {},
   "source": [
    "# Shuffle Partition\n",
    "After sizing your cluster, the next step is adjusting parallelism (partitions) based on data size.\n",
    "\n",
    "Shuffling happens when we do a wide transformation. Spark tries to bring related data across executors into same partition.\n",
    "## **Data Shuffling:**\n",
    "- This is the process of redistributing data across partitions, and typically involves data exchange between executor nodes.\n",
    "- Wide transformations, require shuffling\n",
    "- It requires saving data to disk, sending it over network and reading data from the disk\n",
    "- Data Shuffling can be very expensive\n",
    "- Sometimes it can be mitigated or avoided by even code changes, like for instance avoiding sorting the data\n",
    "- Nevertheless, it's often a \"necessary evil\".\n",
    "\n",
    "\n",
    "**Step:**\n",
    "- Once a file is read in spark it is broken into partitions. p1, p2, p3, p4.\n",
    "- say a shuffle was done (groupBy) in the partition, it will end up moving related data to a single partition, and at the end each partition that holds data is called a `shuffle partition`. \n",
    "- By default spark creates 200 shufflue partitions.\n",
    "- Say you have a cluster of 1000 cores, and after spark reads your data into 200 partitions, this means, 1 core handles 1 partition, which then says 200 cores will process 200, partitions, and this leaves 800 cores idle.\n",
    "  - This will result to slow completion time and underutilization of cluster resource.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad7c7d",
   "metadata": {},
   "source": [
    "## How to set shuffle partition.\n",
    "Scenerio 1: Data per shuffle partition is very large, and we tune it to a reasonable number by calculation a shuffle partition size.\n",
    "\n",
    "Calculation: \n",
    "5 execs and 4 cores; default shuffle partition -> 200 parts; data size to be shuffled 300GB.\n",
    "\n",
    "total_core = 5 x 4 = 20 core;\n",
    "shuffle_partition = 200 parts\n",
    "\n",
    "> Find size data per shuffle parts. = 300 GB / 200 s.p = 1.5gb\n",
    "> this means each core will process 1.5gb of data.\n",
    "\n",
    "But optimal partition size should be between 1MB - 200MB per core. Which mean 1 executor should process 1 - 200MB of data per task.\n",
    "\n",
    "Now we need to tune the number of shuffle partition `spark.sql.shuffle.partition` in other for each core st handle an adequate amount of data.\n",
    "\n",
    "- let's choose 200MB for the number of shuffle partition size, meaning we want 1 cpu to handle a total of 200MB of shuffle partition.\n",
    "(300 GB x 1000 MB) / 200 MB = 1500 S.P\n",
    "\n",
    "Therefore,\n",
    "`spark.sql.shuffle.partition=1500`, this makes 1 cpu process, 200MB.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Scenerio 2: Data per shuffle partition is very small, and we tune it to a reasonable number by calculation a shuffle partition size.\n",
    "\n",
    "Say we have 3 executors, 4 cpu = 12 cores in total;\n",
    "to process 50MB of data using default S.P of 200 parts.\n",
    "\n",
    "Partition size per core = 50/200 -> 250KB; 250kb shared across 12 cores.\n",
    "\n",
    "This is really small and the optimal size is supposed to be within 1MB - 200MB.\n",
    "\n",
    "op 1: change the number of S.P: choose say 10 MB SP.\n",
    "so 50MB / 10MB = 5. so set S.P to 5;\n",
    "But the issue here is, only 5 core out of 12 will process data, this is underutilization even for small data.\n",
    "\n",
    "set `spark.sql.shuffle.partition=5`\n",
    "\n",
    "option 2: 50MB / 12 cores = 4.2MB\n",
    "which means each core will process 4.2MB of data.\n",
    "\n",
    "set `spark.sql.shuffle.partition=12` \n",
    "\n",
    "tuning your executors may not fully solve your slow data processing, there could be issues like Skew (which can be solved with salting or enabling AQE).\n",
    "\n",
    "> spark.sql.shuffle.partitions = 12 sets the number of partitions after Spark performs a shuffle, not the initial partitions from reading the data âœ….\n",
    "\n",
    "\n",
    "**Question:**\n",
    "But can I control the initial read shuffle partition?\n",
    "\n",
    "**Answer:**\n",
    "âœ… Yes, you can control the number of partitions when reading data â€” but it's separate from shuffle partitions `spark.sql.shuffle.partition`.\n",
    "\n",
    "> df = spark.read.option(\"maxPartitionBytes\", value).csv(\"path\")\n",
    "or\n",
    "> spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \n",
    "This controls how Spark splits the input files into initial partitions (before any transformations or shuffles).\n",
    "\n",
    "\n",
    "You can also manually repartition:\n",
    "> df = spark.read.csv(\"path\").repartition(12)\n",
    "12 initial partitions, before any shuffle.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
