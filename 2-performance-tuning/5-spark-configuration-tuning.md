https://youtu.be/mA96gUESVZc?si=OJSMhiUb6DUCHP3N

## âœ… 1. **What is Spark Configuration?**
**Definition:**
Spark configuration refers to the **key-value settings** that control how Spark behaves at runtime. These can be applied when submitting jobs (`spark-submit`), in the code via `SparkConf`, or through default files like `spark-defaults.conf`.

### ğŸ”§ Examples:
* `spark.executor.memory` â€“ memory per executor (e.g., `4g`)
* `spark.executor.instances` â€“ number of executors
* `spark.sql.shuffle.partitions` â€“ number of partitions post-shuffle (default: 200)
* `spark.driver.memory` â€“ memory for driver

### ğŸŸ¨ Purpose:
To set up Spark's **environment** and **execution parameters** before a job runs.

---












## âœ… 2. **What is Spark Tuning?**

**Definition:**
Spark tuning is the **process of adjusting configurations** to achieve **better performance and resource efficiency** based on your workload and cluster setup.

### ğŸ” It includes:
* **Executor tuning** (e.g., memory, cores per executor)
* **Parallelism tuning** (`spark.default.parallelism`)
* **Shuffle tuning** (e.g., using `reduceByKey` over `groupByKey`)
* **Skew mitigation** (handling data imbalance)

### ğŸ¯ Goal:
To **fine-tune Sparkâ€™s execution** so that your jobs are faster, cheaper, and more reliable.

---